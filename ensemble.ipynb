{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./data/X.npy')\n",
    "X_polynomial = np.load('./data/X_polynomial.npy')\n",
    "y = np.load('./data/y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_reg = HuberRegressor(epsilon= 5.09, alpha= 0.0004)\n",
    "ridge_reg = linear_model.Ridge(solver='saga', max_iter=4000, alpha= 0.582)\n",
    "lasso_reg = linear_model.Lasso(max_iter=4000, alpha=0.0038, normalize=False)\n",
    "dt_reg = tree.DecisionTreeRegressor(min_samples_split=7, min_samples_leaf=7, min_weight_fraction_leaf=0.000516, \n",
    "                                                                                             max_features='auto')\n",
    "    \n",
    "pa_reg = PassiveAggressiveRegressor(max_iter=3600, tol=1e-3)\n",
    "xgb_reg = xgb.XGBRegressor(objective=\"reg:linear\", alpha= 0.00244, booster='dart', eta= 0.017326, gamma=0.19504, \n",
    "                           reg_lambda=0.22451, max_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [huber_reg, ridge_reg, lasso_reg, dt_reg, pa_reg, xgb_reg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class ShuffleVoter(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "       scikit-learn based voting aggregation ensembling.\n",
    "       Using bootstrapping creates a set of models, differing only by which data sample they are fed\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        \"\"\"\n",
    "        model - base model ( or a pipeline ) ( unfitted )\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        \n",
    "    def ensemble_predictions(self, predictions, weights, type_=\"linear\"):\n",
    "        \"\"\"\n",
    "        Combines probabilistic class estimates using a variety of strategies.\n",
    "        Linear, harmonic, geometric and rank averaging are supported at this moment. \n",
    "        Insipred by well known Abhishek's kernel on Kaggle \n",
    "        model - base model ( or a pipeline ) ( unfitted )\n",
    "        \"\"\"\n",
    "        assert np.isclose(np.sum(weights), 1.0)\n",
    "        if type_ == \"linear\":\n",
    "            res = np.average(predictions, weights=weights, axis=0)\n",
    "        elif type_ == \"harmonic\":\n",
    "            res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "            return 1 / res\n",
    "        elif type_ == \"geometric\":\n",
    "            numerator = np.average(\n",
    "                [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "            )\n",
    "            res = np.exp(numerator / sum(weights))\n",
    "            return res\n",
    "        elif type_ == \"rank\":\n",
    "            res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "            return res / (len(res) + 1)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def fit( self, X, y, n_boots = 14, test_size = 100 ):\n",
    "        \"\"\"\n",
    "        n_boots - number of bootstrapping iterations ( and respective models built)\n",
    "        \"\"\"\n",
    "        self.clfs  = []\n",
    "        for i, model in zip(range(n_boots), cycle(self.models)):\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split( X, y, test_size=test_size, random_state=3521 + i*10)\n",
    "\n",
    "            pa_clf = model\n",
    "            pa_clf.fit(X_tr, y_tr)\n",
    "\n",
    "            self.clfs.append(pa_clf)\n",
    "\n",
    "    def predict( self, X, ensemble_type = 'rank'):\n",
    "        # TODO: nonuniform weights\n",
    "        \n",
    "        n_boots = len( self.clfs)\n",
    "        preds = [ clf.predict(X) for clf in self.clfs ]\n",
    "        return self.ensemble_predictions( preds, np.ones(n_boots)*(1/float(n_boots)), ensemble_type)\n",
    "    \n",
    "    def predict_proba( self, X, ensemble_type = 'rank' ):\n",
    "        n_boots = len( self.clfs)\n",
    "        preds = [ clf.predict_proba(X) for clf in self.clfs ]\n",
    "        return self.ensemble_predictions( preds, np.ones(n_boots)*(1/float(n_boots)), ensemble_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Fold r2-score: 0.9993761660240382\n",
      "\n",
      "[21:59:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Fold r2-score: 0.99942619924951\n",
      "\n",
      "[21:59:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Fold r2-score: 0.9993646314251734\n",
      "\n",
      "[21:59:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Fold r2-score: 0.9993672975680242\n",
      "\n",
      "[21:59:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Fold r2-score: 0.9993455147473068\n",
      "\n",
      "Cross-validation score: 0.9993759618028106\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "folds_r2_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    y_pred = []\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred.append(model.predict(X_test))\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.mean(y_pred, axis=0)\n",
    "\n",
    "#     y_pred = my_shuffle_voter.predict(X_test)\n",
    "    print(\"Fold r2-score:\", r2_score(y_test, y_pred))\n",
    "#     print(y_test[:10])\n",
    "#     print(y_pred[:10])\n",
    "    print()\n",
    "    folds_r2_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "print(\"Cross-validation score:\", sum(folds_r2_scores)/float(len(folds_r2_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
