{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt.pyll import scope as ho_scope\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file = open('./data/shashlik_61_pulses.txt', 'r')\n",
    "    data = file.readlines()\n",
    "    data = np.array([list(map(float, experiment.split())) for experiment in data])\n",
    "   \n",
    "    X = data[:, 2:]\n",
    "    y_baseline = data[:, 1]\n",
    "    y = data[:, 0]\n",
    "    \n",
    "    \n",
    "    X = np.array([experiment - np.max(experiment) for experiment in X])\n",
    "    X = np.array([experiment/-np.min(experiment) for experiment in X])\n",
    "\n",
    "    y = np.round(y)\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    ## Let's shift each signal so that reference time matches for each signal\n",
    "    mean_ref_time = int(y.mean())\n",
    "    X = np.array([signal_cyclic_shift(signal, mean_ref_time - y[i]) for i, signal in enumerate(X, 0)])\n",
    "    y = np.array([mean_ref_time]*len(y))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_freq_data(X, freq=1, start_point=384):\n",
    "    X_freq = np.concatenate([X[:, start_point::-freq][:, ::-1], X[:, start_point + freq::freq]], axis=1)\n",
    "    return X_freq\n",
    "\n",
    "def signal_cyclic_shift(signal, tau):\n",
    "    signal_start = signal[:-tau]\n",
    "    \n",
    "    new_signal = np.concatenate([signal[-tau:], signal_start])\n",
    "    \n",
    "    return new_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_origin, y_origin = get_data()\n",
    "\n",
    "mean_argmin =  int(np.argmin(X_origin, axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_time(first_impulse, second_impulse, first_ref_time, second_ref_time):\n",
    "    if np.min(first_impulse) < np.min(second_impulse):\n",
    "         return first_ref_time\n",
    "    else:\n",
    "        return second_ref_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_multi_signal(X_origin, y_origin, tau, alpha, to_plot=False):\n",
    "    first_idx, second_idx = np.random.choice(X_origin.shape[0], 2, replace=False)\n",
    "    first_impulse = X_origin[first_idx]\n",
    "    second_impulse = X_origin[second_idx]\n",
    "    \n",
    "    first_ref_time = y_origin[first_idx]\n",
    "    second_ref_time = y_origin[second_idx]\n",
    "    \n",
    "    \n",
    "#     print(\"SHIFT:\", tau)\n",
    "#     print(\"BEFORE SHIFT:\", first_ref_time, second_ref_time)\n",
    "    ### Randomly choose what signal to shift\n",
    "    if random.choice([True, False]):\n",
    "        first_impulse = signal_cyclic_shift(first_impulse, tau)\n",
    "        first_ref_time += tau\n",
    "    else:\n",
    "        second_impulse = signal_cyclic_shift(second_impulse, tau)\n",
    "        second_ref_time += tau\n",
    "    \n",
    "#     print(\"AFTER SHIFT:\", first_ref_time, second_ref_time)\n",
    "    \n",
    "    multi_impulse = first_impulse + second_impulse*alpha\n",
    "    multi_impulse /= -np.min(multi_impulse)\n",
    "    \n",
    "    first_impulse_shifted = signal_cyclic_shift(first_impulse, mean_argmin - np.argmin(first_impulse))\n",
    "    second_impulse_shifted = signal_cyclic_shift(second_impulse, mean_argmin - np.argmin(second_impulse))\n",
    "    multi_impulse_shifted = signal_cyclic_shift(multi_impulse, mean_argmin - np.argmin(multi_impulse))\n",
    "\n",
    "#     print(mean_argmin - np.argmin(multi_impulse))\n",
    "    first_ref_time +=  mean_argmin - np.argmin(multi_impulse)\n",
    "    second_ref_time +=  mean_argmin - np.argmin(multi_impulse)\n",
    "    \n",
    "    if to_plot:\n",
    "        plt.plot(first_impulse)\n",
    "        plt.plot(second_impulse)\n",
    "        plt.plot(multi_impulse_shifted)\n",
    "        plt.legend(['First signal', 'Second signal', 'Sum of signals'])\n",
    "        plt.show()\n",
    "        \n",
    "    ref_time = get_ref_time(first_impulse, second_impulse*alpha, first_ref_time, second_ref_time)\n",
    "    \n",
    "    return {'first_impulse': first_impulse_shifted,\\\n",
    "            'second_impulse': second_impulse_shifted,\\\n",
    "            'ref_time': ref_time,\\\n",
    "            'multi_impulse': multi_impulse_shifted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_data(X_origin, y_origin, tau_range, alpha_range, data_size=1000, to_print=False):    \n",
    "    X = []\n",
    "    y = []\n",
    "    alpha_values = []\n",
    "    tau_values = []\n",
    "    for i in range(data_size):\n",
    "        alpha = random.choice(alpha_range)\n",
    "        tau = random.choice(tau_range)\n",
    "        signal = generate_multi_signal(X_origin, y_origin, tau, alpha)\n",
    "        \n",
    "        \n",
    "        X.append(signal['multi_impulse'])\n",
    "        y.append(signal['ref_time']) ### We have alpha >= 1 so second_ref_time always will be close to 167,\n",
    "                                           ### so we will predict first_ref_time\n",
    "        \n",
    "        alpha_values.append(alpha)\n",
    "        tau_values.append(tau)\n",
    "   \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if to_print:\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"y shape:\", y.shape)\n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "#     plt.scatter(alpha_values, tau_values)\n",
    "#     plt.show()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_range = np.array([np.around(10**i, decimals=4) for i in np.arange(0, 3.1, 0.01)])\n",
    "alpha_range = np.array([np.around(10**i, decimals=4) for i in np.arange(-3, 3.1, 0.1)])\n",
    "tau_range = np.arange(-100, 101, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 1024)\n",
      "y shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_data(X_origin, y_origin, tau_range, alpha_range, data_size=100, to_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [23:11<00:00,  3.91s/it, best loss: 1.0587785976612172]\n",
      "Found minimum after 100 trials:\n",
      "{'alpha': 0.00998778547224398, 'epsilon': 1.242032186307099, 'max_iter': 410.0}\n"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    huber_reg = HuberRegressor(epsilon=space['epsilon'], max_iter=space['max_iter'], alpha=space['alpha'])\n",
    "    scores = cross_validate(huber_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'epsilon':  hp.loguniform('epsilon', low=np.log(1.1), high=np.log(10)),\n",
    "    'max_iter': ho_scope.int(hp.quniform('max_iter', low=100, high=500, q=10)),\n",
    "    'alpha':  hp.loguniform('alpha', low=np.log(0.0001), high=np.log(0.01)),\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:19<00:00,  5.32it/s, best loss: 1.2577307742459154]\n",
      "Found minimum after 100 trials:\n",
      "{'alpha': 0.5134223164532016, 'max_iter': 3700.0, 'solver': 3}\n"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    ridge_reg = linear_model.Ridge(solver=space['solver'], max_iter=space['max_iter'], alpha=space['alpha'])\n",
    "    scores = cross_validate(ridge_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'solver': hp.choice('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "    'max_iter': ho_scope.int(hp.quniform('max_iter', low=1000, high=5000, q=100)),\n",
    "    'alpha':  hp.loguniform('alpha', low=np.log(0.0001), high=np.log(1)),\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:37<00:00,  1.83it/s, best loss: 1.2567995430614787]\n",
      "Found minimum after 100 trials:\n",
      "{'alpha': 0.008499157214207619, 'max_iter': 4700.0, 'normalize': 1}\n"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    lasso_reg = linear_model.Lasso(max_iter=space['max_iter'], alpha=space['alpha'], normalize=space['normalize'])\n",
    "    scores = cross_validate(lasso_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'normalize': hp.choice('normalize', [True, False]),\n",
    "    'max_iter': ho_scope.int(hp.quniform('max_iter', low=1000, high=5000, q=100)),\n",
    "    'alpha':  hp.loguniform('alpha', low=np.log(0.0001), high=np.log(1)),\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:41<00:00,  8.80it/s, best loss: 1.5376176601176594]\n",
      "Found minimum after 100 trials:\n",
      "{'max_features': 0, 'max_iter': 38.0, 'min_samples_leaf': 9.0, 'min_samples_split': 9.0, 'min_weight_fraction_leaf': 0.017959228408691214}\n"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    dt_reg = tree.DecisionTreeRegressor(max_depth=space['max_depth'], min_samples_split=space['min_samples_split'],\n",
    "                                       min_samples_leaf=space['min_samples_leaf'], min_weight_fraction_leaf=\n",
    "                                        space['min_weight_fraction_leaf'], max_features=space['max_features'])\n",
    "    scores = cross_validate(dt_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'max_depth':  ho_scope.int(hp.quniform('max_iter', low=4, high=100, q=2)),\n",
    "    'min_samples_split': ho_scope.int(hp.quniform('min_samples_split', low=2, high=10, q=1)),\n",
    "    'min_samples_leaf':  ho_scope.int(hp.quniform('min_samples_leaf', low=1, high=10, q=1)),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0, 0.5),\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2'])\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passive aggresive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:38<00:00,  8.38it/s, best loss: 2.5793505639871768]\n",
      "Found minimum after 100 trials:\n",
      "{'c': 0.18503178295924097, 'max_iter': 1700.0, 'tol': 0.00015132248349779718, 'verbose': 66.0}\n"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    pa_reg = PassiveAggressiveRegressor(max_iter=space['max_iter'], tol=space['max_iter'], \n",
    "                                       C = space['C'])\n",
    "    scores = cross_validate(pa_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'max_iter': ho_scope.int(hp.quniform('max_iter', low=1000, high=5000, q=100)),\n",
    "    'tol': hp.loguniform('tol', low=np.log(0.000001), high=np.log(0.001)),\n",
    "    'verbose': ho_scope.int(hp.quniform('verbose', low=1, high=100, q=2)),\n",
    "    'C':  hp.loguniform('c', low=np.log(0.0001), high=np.log(10)),\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 383/500 [35:08<14:27,  7.41s/it, best loss: 1.6928302001953124]"
     ]
    }
   ],
   "source": [
    "def f(space):\n",
    "    xgb_reg = xgb.XGBRegressor(objective=\"reg:linear\", booster=space['booster'], eta=space['eta'], \n",
    "                               gamma=space['gamma'], max_depth=space['max_depth'], reg_lambda=space['lambda'],\n",
    "                               alpha=space['alpha'], verbosity=0)\n",
    "    scores = cross_validate(xgb_reg, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -scores['test_score'].mean()\n",
    "    \n",
    "space = {\n",
    "    'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "    'eta': hp.loguniform('eta', low=np.log(0.001), high=np.log(1)),\n",
    "    'gamma': hp.loguniform('gamma', low=np.log(0.001), high=np.log(100)),\n",
    "    'max_depth': ho_scope.int(hp.quniform('max_depth', low=5, high=50, q=2)),\n",
    "    'lambda': hp.loguniform('lambda', low=np.log(0.001), high=np.log(10)),\n",
    "    'alpha':  hp.loguniform('alpha', low=np.log(0.001), high=np.log(10)),\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn=f,  # \"Loss\" function to minimize\n",
    "    space=space,  # Hyperparameter space\n",
    "    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "    max_evals=500  # Perform 100 trials\n",
    ")\n",
    "\n",
    "print(\"Found minimum after 100 trials:\")\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
