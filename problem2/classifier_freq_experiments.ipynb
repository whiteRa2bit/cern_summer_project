{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file = open('./data/shashlik_61_pulses.txt', 'r')\n",
    "    data = file.readlines()\n",
    "    data = np.array([list(map(float, experiment.split())) for experiment in data])\n",
    "   \n",
    "    X = data[:, 2:]\n",
    "    y_baseline = data[:, 1]\n",
    "    y = data[:, 0]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_freq_data(X, freq=20, start_point=384):\n",
    "    X_freq = np.concatenate([X[:, start_point::-freq], X[:, start_point::freq]], axis=1)\n",
    "    return X_freq\n",
    "\n",
    "def signal_cyclic_shift(signal, tau):\n",
    "    signal_start = signal[:-tau]\n",
    "    \n",
    "    new_signal = np.concatenate([signal[-tau:], signal_start])\n",
    "    \n",
    "    return new_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_signal(X_origin, tau, alpha, to_plot=False):\n",
    "    first_idx, second_idx = np.random.choice(X_origin.shape[0], 2, replace=False)\n",
    "    first_impulse = X_origin[first_idx]\n",
    "    second_impulse = X_origin[second_idx]\n",
    "    \n",
    "    second_impulse = signal_cyclic_shift(second_impulse, tau)\n",
    "    \n",
    "    multi_impulse = first_impulse + second_impulse*alpha\n",
    "    multi_impulse /= -np.min(multi_impulse)\n",
    "    \n",
    "    first_impulse_shifted = signal_cyclic_shift(first_impulse, mean_argmin - np.argmin(first_impulse))\n",
    "    second_impulse_shifted = signal_cyclic_shift(second_impulse, mean_argmin - np.argmin(second_impulse))\n",
    "    multi_impulse_shifted = signal_cyclic_shift(multi_impulse, mean_argmin - np.argmin(multi_impulse))\n",
    "\n",
    "    if to_plot:\n",
    "        plt.plot(first_impulse_shifted)\n",
    "        plt.plot(second_impulse_shifted)\n",
    "        plt.plot(multi_impulse_shifted)\n",
    "        plt.legend(['First signal', 'Second signal', 'Sum of signals'])\n",
    "        plt.show()\n",
    "        \n",
    "    return {'first': first_impulse_shifted,\\\n",
    "            'second': second_impulse_shifted,\\\n",
    "            'multi': multi_impulse_shifted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_data(X_origin, tau, alpha, freq, data_size=3000, to_print=False):\n",
    "    pos_size = int(data_size/2)\n",
    "    neg_size = data_size - pos_size\n",
    "    \n",
    "    X_positive = np.array([generate_multi_signal(X_origin, tau, alpha)['first'] for i in range(pos_size)])\n",
    "    y_positive = np.ones(pos_size)\n",
    "    \n",
    "    X_negative = np.array([generate_multi_signal(X_originm tau, alpha)['multi'] for i in range(neg_size)])\n",
    "    y_negative = np.zeros(neg_size)\n",
    "    \n",
    "    X = np.concatenate([X_positive, X_negative])\n",
    "    y = np.concatenate([y_positive, y_negative])\n",
    "   \n",
    "    if to_print:\n",
    "        print(\"X positive shape:\", X_positive.shape)\n",
    "        print(\"y positive shape:\", y_positive.shape)\n",
    "        print(\"X negative shape:\", X_negative.shape)\n",
    "        print(\"y negative shape:\", y_negative.shape)\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"y shape:\", y.shape)\n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_pipelines(hyperparams = {'svc_c': 0.1, 'mlp_max_iter': 1000, 'kn_n': 5, 'pa_c': 0.0001}):\n",
    "    SVC_pipeline = Pipeline([\n",
    "                    ('clf', LinearSVC(C=hyperparams['svc_c'])),\n",
    "                ])\n",
    "\n",
    "\n",
    "    MLP_pipeline = Pipeline([\n",
    "                    ('clf', MLPClassifier(max_iter=hyperparams['mlp_max_iter'])),\n",
    "                ])\n",
    "\n",
    "    KN_pipeline = Pipeline([\n",
    "                    ('clf', KNeighborsClassifier(n_neighbors=hyperparams['kn_n'])),\n",
    "                ])\n",
    "    \n",
    "    PA_pipeline = Pipeline([\n",
    "                ('clf', PassiveAggressiveClassifier(C = hyperparams['pa_c'], max_iter=5000, tol=1e-3)),\n",
    "            ])\n",
    "\n",
    "    pipelines = {'svc': SVC_pipeline, 'kn': KN_pipeline, 'pa': PA_pipeline,  'mlp': MLP_pipeline}\n",
    "    \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(alpha_values, tau_values):\n",
    "    scores_dict = {}\n",
    "    \n",
    "    for tau in tqdm.tqdm(tau_values):\n",
    "        scores_dict[tau] = dict(zip(alpha_values, np.zeros(len(alpha_values))))\n",
    "        for alpha in alpha_values:\n",
    "            X, y = prepare_data(tau, alpha, data_size=2000)\n",
    "            pipelines = get_pipelines()\n",
    "\n",
    "            scoring = {'accuracy' : make_scorer(accuracy_score)}\n",
    "            scores = cross_validate(pipelines['kn'], X, y, scoring=scoring, cv=5)\n",
    "\n",
    "            scores_dict[tau][alpha] = scores[\"test_accuracy\"].mean()\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('./data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name ):\n",
    "    with open('./data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_arrays(scores_dict):\n",
    "    x, y, z = [], [], []\n",
    "    for tau, alpha_dict in scores_dict.items():\n",
    "        for alpha, score in alpha_dict.items():\n",
    "            x.append(tau)\n",
    "            y.append(alpha)\n",
    "            z.append(score)\n",
    "            \n",
    "    return [x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "matplotlib.use('Qt4Agg')\n",
    "\n",
    "def plot_accuracy_3d(scores_dict):\n",
    "    x, y, z = dict_to_arrays(scores_dict)\n",
    "    alpha_values = list(scores_dict[list(scores_dict.keys())[0]].keys())\n",
    "    \n",
    "    fig1 = matplotlib.pyplot.figure(figsize=(7, 5))\n",
    "    ax1 = Axes3D(fig1)\n",
    "\n",
    "    ax1.set_yticks(np.log10(alpha_values[::10]))\n",
    "    ax1.set_yticklabels(alpha_values[::10])\n",
    "\n",
    "    ax1.set_xlabel(r\"${\\tau}$\", fontsize=15)\n",
    "    ax1.set_ylabel(r\"${\\alpha}$\", fontsize=15)\n",
    "    ax1.set_zlabel('Accuracy', fontsize=15)\n",
    "\n",
    "    ax1.view_init(30,150)\n",
    "    ax1.plot(10*x, np.log10(10*y), 10*z, 'ro', color='b', linewidth=3)\n",
    "\n",
    "    # for angle in range(0, 220):\n",
    "    #     ax1.view_init(30, angle)\n",
    "    #     plt.draw()\n",
    "    #     plt.pause(.001)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def plot_color_map(scores_dict, alpha_values, tau_values, alpha_freq=10, tau_freq=4):\n",
    "    _, _, z = dict_to_arrays(scores_dict)\n",
    "    z = np.array(z)\n",
    "    Z = z.reshape((len(tau_values), len(alpha_values)))\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    ax = plt.gca()\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Two signals ratio\", fontsize=12)\n",
    "    plt.ylabel(\"Delta t [ns]\", fontsize=12)\n",
    "    plt.xticks([alpha_freq*i for i in range(len(alpha_values[::alpha_freq]))], alpha_values[::alpha_freq])\n",
    "    plt.yticks([tau_freq*i for i in range(len(tau_values[::tau_freq]))], [0.2*el for el in tau_values[::tau_freq]])\n",
    "\n",
    "    im = ax.imshow(Z, interpolation='bilinear', cmap='spring', aspect='auto')\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = np.array([np.around(10**i, decimals=4) for i in np.arange(-3, 3.1, 0.1)])\n",
    "tau_values = range(-100, 110, 10)\n",
    "pipelines = get_pipelines()\n",
    "\n",
    "for freq in range(2, 40, 2):\n",
    "    X_origin, y_origin = get_data()\n",
    "\n",
    "    X_origin = np.array([experiment - np.max(experiment) for experiment in X_origin])\n",
    "    X_origin = np.array([experiment/-np.min(experiment) for experiment in X_origin])\n",
    "    X_origin = get_freq_data(X_origin)\n",
    "    \n",
    "    y_origin = np.round(y_origin)\n",
    "    y_origin = y_origin.astype(int)\n",
    "    \n",
    "    pipelines\n",
    "\n",
    "    for model_name, model in pipelines.items():\n",
    "        print(\"Model name:\", model_name)\n",
    "\n",
    "        scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "               'precision' : make_scorer(precision_score),\n",
    "               'recall' : make_scorer(recall_score), \n",
    "               'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "        scores = cross_validate(model, X, y, scoring=scoring, cv=10)\n",
    "        for score in scoring.keys():\n",
    "            print(score, \"%0.5f (+/- %0.5f)\" % (scores[\"test_\" + score].mean(), scores[\"test_\" + score].std() * 2))\n",
    "        print('------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
